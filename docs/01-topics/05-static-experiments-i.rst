Static experiments (I)
======================

In PsyNet, we define a *static experiment* as an experiment where the stimuli
(or, more technically, the 'nodes') stay fixed over time.
Many experiments fit into this mould, including many of our demos:

- :doc:`01-simple-rating <../02-demos/pipelines/01-simple-rating>`: Participants rate audio stimuli for specified attributes;
- :doc:`02-tapping <../02-demos/pipelines/02-tapping>`: Participants tap to the beat of musical stimuli;
- :doc:`04-similarity <../02-demos/pipelines/04-similarity>`: Participants rate the similarity of audio stimuli;
- :doc:`05-timed-push-buttons <../02-demos/pipelines/05-timed-push-buttons>`: Participants press buttons at interesting moments in audio stimuli.

.. note::

    In the :doc:`03-step-tag <../02-demos/pipelines/03-step-tag>` demo,
    the stimuli comprise not only audio files but also the tags generated by previous participants.
    This means that it is not a static paradigm, but rather a 'chain' paradigm.

This exercise will introduce you to the key concepts of static experiments
by working through the implementation of the :doc:`simple rating <../02-demos/pipelines/01-simple-rating>` pipeline.

The :doc:`simple rating <../02-demos/pipelines/01-simple-rating>` implementation begins with the following
implementation of ``get_timeline``:

.. code-block:: python

    def get_timeline():
        return Timeline(
            InfoPage(
                """
                In this experiment you will hear some sounds.
                Your task will be to rate them from 1 to 5 on several scales.
                """,
                time_estimate=5,
            ),
            StaticTrialMaker(
                id_="ratings",
                trial_class=CustomTrial,
                nodes=get_nodes(),
                expected_trials_per_participant="n_nodes",
            ),
            InfoPage(
                "Thank you for your participation",
                time_estimate=5,
            ),
        )

We are familiar already with info pages from the :doc:`03-pages` topic.
However, the ``StaticTrialMaker`` object is new. So, what exactly is a trial maker?

A trial maker is a special kind of module (see :doc:`04-timeline`)
that administers trials to the participant.
In the above code, the trial maker has been instantiated with the ID `"ratings"`,
and with the trial class ``CustomTrial``.

Trial
-----

In PsyNet, a trial is conceptualized as a basic repeatable unit of an experiment's design.
In many experiments, a trial corresponds to a single page, presenting a single stimulus to the participant
and recording a single response; however, it is also possible for a trial to comprise multiple pages
and record multiple responses.

When we implement an experiment with a trial maker, we normally need to implement our own
custom trial class. This is done by subclassing the relevant base trial class in PsyNet.
Here we are implementing a static experiment, so we subclass ``StaticTrial``:

.. code-block:: python

    class CustomTrial(StaticTrial):
        time_estimate = 10

        def show_trial(self, experiment, participant):
            return ModularPage(
                "ratings",
                AudioPrompt(
                    self.assets["stimulus_audio"],
                    "Please rate the sound. You can replay it as many times as you like.",
                    controls="Play",
                ),
                MultiRatingControl(
                    RatingScale(
                        name="brightness",
                        values=5,
                        title="Brightness",
                        min_description="Dark",
                        max_description="Bright",
                    ),
                    RatingScale(
                        name="roughness",
                        values=5,
                        title="Roughness",
                        min_description="Smooth",
                        max_description="Rough",
                    ),
                ),
                events={
                    "submitEnable": Event(is_triggered_by="promptEnd"),
                },
            )

This is a relatively simple implementation that just does two things:
it provides a ``time_estimate`` parameter, estimating the duration of the trial in seconds,
and it provides a ``show_trial`` method, which determines the page that is shown to the participant.
In this case, the ``show_trial`` method returns a fairly simple modular page (see :doc:`03-pages`).

Many aspects of this trial are fixed by definition.
The main thing that can differ is ``self.assets["stimulus_audio"]``, which determines the audio that is played to
the participant.

Where does ``self.assets["stimulus_audio"]`` come from?
To explain this, we will need to introduce the notion of *nodes*.

Nodes
-----

Nodes determine the structure of static experiments (and indeed chain experiments, when we get to them).
A node is an object that generates trials.
Importantly, the trials inherit key attributes from their parent nodes.
In this case, the trial is inheriting its parent's assets.
Trials also inherit their parent's *definition*, a dictionary of attributes that can also be used to
customize ``show_trial``.

Trial makers are typically initialized with collections of nodes.
In our case, these nodes are generated by the ``get_nodes`` function:

.. code-block:: python

    STIMULUS_DIR = Path("data/instrument_sounds")
    STIMULUS_PATTERN = "*.mp3"

    def get_nodes():
        return [
            StaticNode(
                definition={
                    "stimulus_name": path.stem
                },
                assets={
                    "stimulus_audio": asset(path, cache=True),  # reuse the uploaded file between deployments
                },
            )
            for path in STIMULUS_DIR.glob(STIMULUS_PATTERN)
        ]

.. note::

    This is an example of *list comprehension* syntax, something which is fairly idiosyncratic to Python.
    If it's not familiar, we recommend learning about it online before proceeding further.

Here we've generated one node per audio file of nodes by listing the ``.mp3`` files in ``data/instrument_sounds``.
There's one node per file; each node has a simple definition comprising just the file stem,
and a single asset corresponding to the file itself.
The generated trial will be able to access the inherited asset via ``self.assets``
and the inherited definition via ``self.definition``.

The default behavior of a ``StaticTrialMaker`` is to administer a sequence of trials to the participant
where each successive trial is generated from a different node. By default, the nodes are chosen such that trials
accumulate evenly across nodes; in other words, we make sure that all nodes have 10 trials before allowing
any of the nodes to have 11 trials. However, this behavior is customizable in many different ways,
by setting parameters in either the ``StaticNode`` objects or the ``StaticTrialMaker`` object (see below).

Blocks
------

It is possible to assign each node to a block.
For example, we could write something like this:

.. code-block:: python

    def get_nodes():
        return [
            StaticNode(
                definition={"instrument": "violin"},
                block="strings",
            ),
            StaticNode(
                definition={"instrument": "cello"},
                block="strings",
            ),
            StaticNode(
                definition={"instrument": "double bass"},
                block="strings",
            ),
            StaticNode(
                definition={"instrument": "trumpet"},
                block="brass",
            ),
            StaticNode(
                definition={"instrument": "horn"},
                block="brass",
            ),
            StaticNode(
                definition={"instrument": "tuba"},
                block="brass",
            )
        ]

Here we have created a node for each instrument,
and assigned the instrument to a block corresponding to the instrument family (either strings or brass).
This means that PsyNet will 'block' the presentation of the stimuli, i.e. the participant will start
with stimuli from one family, then move to the next family, and so on.
This can be useful in certain experiments where you want participants to focus on subtle differences within
stimulus families rather than being distracted by differences between families.

By default, the block order will be randomized for each participant.
However, this behavior can be customized by creating a custom trial maker subclass
and overriding the ``choose_block_order`` method.
For example:

.. code-block:: python

    class CustomTrialMaker(StaticTrialMaker):
        def choose_block_order(self, experiment, participant, blocks):
            # Take the blocks in alphabetical order
            return sorted(blocks)

    CustomTrialMaker(
        id_="ratings",
        nodes=get_nodes,
        ...
    )

This technique can also be useful if you want to fix the order of stimuli in advance across all participants.
You would use logic like this:

.. code-block:: python

    def get_nodes():
        return [
            StaticNode(
                definition={"instrument": instrument},
                block=str(i)
            )
            for i, instrument in enumerate(["violin", "viola", "guitar", ...])
        ]

    class CustomTrialMaker(StaticTrialMaker):
        def choose_block_order(self, experiment, participant, blocks):
            # Present the stimuli in ascending numeric order of block.
            return sorted(blocks, key=int)


Participant groups
------------------

In an analogous fashion, it is possible to associate each node with a participant group.

.. code-block:: python
    [
        StaticNode(
            definition={"instrument": "trumpet"},
            participant_group="brass_players",
        ),
        StaticNode(
            definition={"instrument": "violin"},
            participant_group="string_players",
        ),
    ]

These nodes will then only be visited by participants within those respective participant groups.

By default, participants are randomly assigned to the participant groups defined within the node collection.
However, it is also possible to define some logic for assigning participants to groups.
Confusingly, the process is slightly different to how we customize block order assignment.
Rather than create a custom subclass, we instead pass a lambda function to the trial maker constructor,
something like this:

.. code-block:: python

    StaticTrialMaker(
        id_="ratings",
        nodes=get_nodes,
        choose_participant_group=lambda participant: participant.var.instrument_family
        ...
    )

The function should return a string corresponding to the group chosen for that participant.

Trial maker parameters
----------------------

There are a variety of other parameters that can be passed to the static trial maker.
Some of these are compulsory; others provide optional avenues for customization.
Here's a list of some key parameters, but for the full set, you should inspect the
static trial maker documentation.

- ``expected_trials_per_participant`` -
    Expected number of trials that each participant will complete.
    This is used for timeline/progress estimation purposes.
    This can either be an integer, or the string ``"n_nodes"``,
    which will be read as referring to the number of provided nodes.
- ``max_trials_per_participant``
    Maximum number of trials that each participant may complete (optional);
    once this number is reached, the participant will move on
    to the next stage in the timeline.
    This can either be an integer, or the string ``"n_nodes"``,
    which will be read as referring to the number of provided nodes.
- ``max_trials_per_block``
    Determines the maximum number of trials that a participant will be allowed to experience in each block,
    including failed trials. Note that this number does not include repeat trials.
- ``allow_repeated_nodes``
    Determines whether the participant can be administered the same node more than once.
- ``max_unique_nodes_per_block``
    Determines the maximum number of unique nodes that a participant will be allowed to experience
    in each block. Once this quota is reached, the participant will be forced to repeat
    previously experienced nodes.
- ``balance_across_nodes``
    If ``True`` (default), active balancing across participants is enabled, meaning that
    node selection favours nodes that have been presented fewest times to any participant
    in the experiment, excluding failed trials.
- ``check_performance_at_end``
    If ``True``, the participant's performance
    is evaluated at the end of the series of trials (see ``TrialMaker.performance_check``).
    Defaults to ``False``.
- ``check_performance_every_trial``
    If ``True``, the participant's performance
    is evaluated after each trial (see ``TrialMaker.performance_check``).
    Defaults to ``False``.
- ``n_repeat_trials``
    Number of repeat trials to present to the participant. These trials
    are typically used to estimate the reliability of the participant's
    responses. Repeat trials are presented at the end of the trial maker,
    after all blocks have been completed.
    Defaults to 0.

Performance checks
------------------

It is possible to implement automated performance checks for trial makers.
A performance check assesses the trials that the participant has completed,
gives the participant a score, and decides whether or not that participant should be failed.
Typically a failed participant would be ejected from the experiment at that point.
This is helpful for implementing performance-based screening tasks.

To implement a performance check, one needs to create a custom subclass for the trial maker,
and define a custom ``performance_check`` method. Arbitrary logic is possible here,
but a straightforward pattern is to override the trial method ``score_answer``,
and then sum up the resulting scores in the ``performance_check`` method.
Something like this:

.. code-block:: python

    class CustomTrial(StaticTrial):
        def score_answer(self, answer, definition):
            return int(answer == definition["correct_answer"])

    class CustomTrialMaker(StaticTrialMaker):
        threshold_score = 5

        def performance_check(self, experiment, participant, participant_trials):
            total_score = sum(t.score for t in participant_trials)
            return {
                "score": total_score,
                "passed": total_score > self.threshold_score
            }

    CustomTrialMaker(
        id_="ratings",
        nodes=get_nodes,
        check_performance_at_end=True,
    )

Of course, one could equivalently define the threshold score in terms of a mean trial score
rather than a total trial score, so as to be invariant to the total number of trials.
Also, one needs to indicate when instantiating the trial maker when the performance check should be run;
above we've specified ``check_performance_at_end=True``, which means that the performance check will be run
after the participant has completed the trial maker.
